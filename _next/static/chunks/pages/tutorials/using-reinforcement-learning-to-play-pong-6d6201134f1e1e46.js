(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[227],{3642:function(s,a,n){(window.__NEXT_P=window.__NEXT_P||[]).push(["/tutorials/using-reinforcement-learning-to-play-pong",function(){return n(1295)}])},1295:function(s,a,n){"use strict";n.r(a),n.d(a,{default:function(){return d}});var p=n(5893),e=n(1618),l=n(6485),t={html:'<main>\n <div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">\n  <div class="jp-Cell-inputWrapper" tabindex="0">\n   <div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">\n   </div>\n   <div class="jp-InputArea jp-Cell-inputArea">\n    <div class="jp-InputPrompt jp-InputArea-prompt">\n    </div>\n    <div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">\n     <h1>\n      Using Reinforcement Learning to Play Pong\n      <a class="anchor-link" href="#Using-Reinforcement-Learning-to-Play-Pong">\n       \xb6\n      </a>\n     </h1>\n     <p>\n      This tutorial demonstrates using reinforcement learning to train an agent to play Pong.  This task isn\'t directly related to chemistry, but video games make an excellent demonstration of reinforcement learning techniques.\n     </p>\n     <p>\n      <img alt="title" src="assets/pong.png"/>\n     </p>\n     <h2>\n      Colab\n      <a class="anchor-link" href="#Colab">\n       \xb6\n      </a>\n     </h2>\n     <p>\n      This tutorial and the rest in this sequence can be done in Google Colab (although the visualization at the end doesn\'t work correctly on Colab, so you might prefer to run this tutorial locally). If you\'d like to open this notebook in colab, you can use the following link.\n     </p>\n     <p>\n      <a href="https://colab.research.google.com/github/deepchem/deepchem/blob/master/examples/tutorials/Using_Reinforcement_Learning_to_Play_Pong.ipynb">\n       <img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/>\n      </a>\n     </p>\n    </div>\n   </div>\n  </div>\n </div>\n <div class="jp-Cell jp-CodeCell jp-Notebook-cell">\n  <div class="jp-Cell-inputWrapper" tabindex="0">\n   <div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">\n   </div>\n   <div class="jp-InputArea jp-Cell-inputArea">\n    <div class="jp-InputPrompt jp-InputArea-prompt">\n     In\xa0[1]:\n    </div>\n    <div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">\n     <div class="cm-editor cm-s-jupyter">\n      <div class="highlight hl-ipython3">\n       <pre class="overflow-x-scroll font-mono"><span></span><span class="o">!</span>pip install --pre deepchem\n<span class="kn">import</span> <span class="nn">deepchem</span>\n<span class="n">deepchem</span><span class="o">.</span><span class="n">__version__</span>\n</pre>\n      </div>\n     </div>\n    </div>\n   </div>\n  </div>\n  <div class="jp-Cell-outputWrapper">\n   <div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">\n   </div>\n   <div class="jp-OutputArea jp-Cell-outputArea">\n    <div class="jp-OutputArea-child">\n     <div class="jp-OutputPrompt jp-OutputArea-prompt">\n     </div>\n     <div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">\n      <pre class="overflow-x-scroll font-mono">Requirement already satisfied: deepchem in c:\\users\\hp\\deepchem_2 (2.8.1.dev20240501183346)\nRequirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from deepchem) (1.3.2)\nRequirement already satisfied: numpy&gt;=1.21 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from deepchem) (1.26.4)\nRequirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages\\pandas-2.2.1-py3.10-win-amd64.egg (from deepchem) (2.2.1)\nRequirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from deepchem) (1.4.1.post1)\nRequirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from deepchem) (1.12)\nRequirement already satisfied: scipy&gt;=1.10.1 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from deepchem) (1.12.0)\nRequirement already satisfied: rdkit in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages\\rdkit-2023.9.5-py3.10-win-amd64.egg (from deepchem) (2023.9.5)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from pandas-&gt;deepchem) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages\\pytz-2024.1-py3.10.egg (from pandas-&gt;deepchem) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages\\tzdata-2024.1-py3.10.egg (from pandas-&gt;deepchem) (2024.1)\nRequirement already satisfied: Pillow in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from rdkit-&gt;deepchem) (10.2.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from scikit-learn-&gt;deepchem) (3.3.0)\nRequirement already satisfied: mpmath&gt;=0.19 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from sympy-&gt;deepchem) (1.3.0)\nRequirement already satisfied: six&gt;=1.5 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;deepchem) (1.16.0)\n</pre>\n     </div>\n    </div>\n    <div class="jp-OutputArea-child">\n     <div class="jp-OutputPrompt jp-OutputArea-prompt">\n     </div>\n     <div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">\n      <pre class="overflow-x-scroll font-mono">No normalization for SPS. Feature removed!\nNo normalization for AvgIpc. Feature removed!\n</pre>\n     </div>\n    </div>\n    <div class="jp-OutputArea-child">\n     <div class="jp-OutputPrompt jp-OutputArea-prompt">\n     </div>\n     <div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">\n      <pre class="overflow-x-scroll font-mono">WARNING:tensorflow:From c:\\Users\\HP\\anaconda3\\envs\\deep\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\nWARNING:tensorflow:From c:\\Users\\HP\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\nInstructions for updating:\nexperimental_relax_shapes is deprecated, use reduce_retracing instead\n</pre>\n     </div>\n    </div>\n    <div class="jp-OutputArea-child">\n     <div class="jp-OutputPrompt jp-OutputArea-prompt">\n     </div>\n     <div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">\n      <pre class="overflow-x-scroll font-mono">Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named \'dgl\'\nSkipped loading modules with transformers dependency. No module named \'transformers\'\ncannot import name \'HuggingFaceModel\' from \'deepchem.models.torch_models\' (c:\\users\\hp\\deepchem_2\\deepchem\\models\\torch_models\\__init__.py)\nSkipped loading modules with pytorch-lightning dependency, missing a dependency. No module named \'lightning\'\nSkipped loading some Jax models, missing a dependency. No module named \'jax\'\n</pre>\n     </div>\n    </div>\n    <div class="jp-OutputArea-child jp-OutputArea-executeResult">\n     <div class="jp-OutputPrompt jp-OutputArea-prompt">\n      Out[1]:\n     </div>\n     <div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">\n      <pre class="overflow-x-scroll font-mono">\'2.8.1.dev\'</pre>\n     </div>\n    </div>\n   </div>\n  </div>\n </div>\n <div class="jp-Cell jp-CodeCell jp-Notebook-cell">\n  <div class="jp-Cell-inputWrapper" tabindex="0">\n   <div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">\n   </div>\n   <div class="jp-InputArea jp-Cell-inputArea">\n    <div class="jp-InputPrompt jp-InputArea-prompt">\n     In\xa0[2]:\n    </div>\n    <div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">\n     <div class="cm-editor cm-s-jupyter">\n      <div class="highlight hl-ipython3">\n       <pre class="overflow-x-scroll font-mono"><span></span><span class="o">!</span>pip install <span class="s2">"gym[atari,accept-rom-license]"</span>\n</pre>\n      </div>\n     </div>\n    </div>\n   </div>\n  </div>\n  <div class="jp-Cell-outputWrapper">\n   <div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">\n   </div>\n   <div class="jp-OutputArea jp-Cell-outputArea">\n    <div class="jp-OutputArea-child">\n     <div class="jp-OutputPrompt jp-OutputArea-prompt">\n     </div>\n     <div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">\n      <pre class="overflow-x-scroll font-mono">Requirement already satisfied: gym[accept-rom-license,atari] in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (0.26.2)\nRequirement already satisfied: numpy&gt;=1.18.0 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from gym[accept-rom-license,atari]) (1.26.4)\nRequirement already satisfied: cloudpickle&gt;=1.2.0 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from gym[accept-rom-license,atari]) (3.0.0)\nRequirement already satisfied: gym-notices&gt;=0.0.4 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from gym[accept-rom-license,atari]) (0.0.8)\nRequirement already satisfied: ale-py~=0.8.0 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from gym[accept-rom-license,atari]) (0.8.1)\nRequirement already satisfied: autorom~=0.4.2 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2; extra == "accept-rom-license"-&gt;gym[accept-rom-license,atari]) (0.4.2)\nRequirement already satisfied: importlib-resources in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from ale-py~=0.8.0-&gt;gym[accept-rom-license,atari]) (6.4.0)\nRequirement already satisfied: typing-extensions in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from ale-py~=0.8.0-&gt;gym[accept-rom-license,atari]) (4.9.0)\nRequirement already satisfied: click in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from autorom~=0.4.2-&gt;autorom[accept-rom-license]~=0.4.2; extra == "accept-rom-license"-&gt;gym[accept-rom-license,atari]) (8.1.7)\nRequirement already satisfied: requests in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from autorom~=0.4.2-&gt;autorom[accept-rom-license]~=0.4.2; extra == "accept-rom-license"-&gt;gym[accept-rom-license,atari]) (2.31.0)\nRequirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from autorom~=0.4.2-&gt;autorom[accept-rom-license]~=0.4.2; extra == "accept-rom-license"-&gt;gym[accept-rom-license,atari]) (4.66.2)\nRequirement already satisfied: AutoROM.accept-rom-license in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2; extra == "accept-rom-license"-&gt;gym[accept-rom-license,atari]) (0.6.1)\nRequirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from click-&gt;autorom~=0.4.2-&gt;autorom[accept-rom-license]~=0.4.2; extra == "accept-rom-license"-&gt;gym[accept-rom-license,atari]) (0.4.6)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from requests-&gt;autorom~=0.4.2-&gt;autorom[accept-rom-license]~=0.4.2; extra == "accept-rom-license"-&gt;gym[accept-rom-license,atari]) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from requests-&gt;autorom~=0.4.2-&gt;autorom[accept-rom-license]~=0.4.2; extra == "accept-rom-license"-&gt;gym[accept-rom-license,atari]) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\\users\\hp\\anaconda3\\envs\\deep\\lib\\site-packages (from requests-&gt;autorom~=0.4.2-&gt;autorom[accept-rom-license]~=0.4.2; extra == "accept-rom-license"-&gt;gym[accept-rom-license,atari]) (2.2.1)\nRequirement already satisfied: certifi&gt;=2017.4.17 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from requests-&gt;autorom~=0.4.2-&gt;autorom[accept-rom-license]~=0.4.2; extra == "accept-rom-license"-&gt;gym[accept-rom-license,atari]) (2022.5.18.1)\n</pre>\n     </div>\n    </div>\n   </div>\n  </div>\n </div>\n <div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">\n  <div class="jp-Cell-inputWrapper" tabindex="0">\n   <div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">\n   </div>\n   <div class="jp-InputArea jp-Cell-inputArea">\n    <div class="jp-InputPrompt jp-InputArea-prompt">\n    </div>\n    <div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">\n     <h2>\n      Reinforcement Learning\n      <a class="anchor-link" href="#Reinforcement-Learning">\n       \xb6\n      </a>\n     </h2>\n     <p>\n      Reinforcement learning involves an\n      <em>\n       agent\n      </em>\n      that interacts with an\n      <em>\n       environment\n      </em>\n      .  In this case, the environment is the video game and the agent is the player.  By trial and error, the agent learns a\n      <em>\n       policy\n      </em>\n      that it follows to perform some task (winning the game).  As it plays, it receives\n      <em>\n       rewards\n      </em>\n      that give it feedback on how well it is doing.  In this case, it receives a positive reward every time it scores a point and a negative reward every time the other player scores a point.\n     </p>\n     <p>\n      The first step is to create an\n      <code>\n       Environment\n      </code>\n      that implements this task.  Fortunately,\nOpenAI Gym already provides an implementation of Pong (and many other tasks appropriate\nfor reinforcement learning).  DeepChem\'s\n      <code>\n       GymEnvironment\n      </code>\n      class provides an easy way to\nuse environments from OpenAI Gym.  We could just use it directly, but in this case we\nsubclass it and preprocess the screen image a little bit to make learning easier.\n     </p>\n    </div>\n   </div>\n  </div>\n </div>\n <div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">\n  <div class="jp-Cell-inputWrapper" tabindex="0">\n   <div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">\n   </div>\n   <div class="jp-InputArea jp-Cell-inputArea">\n    <div class="jp-InputPrompt jp-InputArea-prompt">\n     In\xa0[3]:\n    </div>\n    <div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">\n     <div class="cm-editor cm-s-jupyter">\n      <div class="highlight hl-ipython3">\n       <pre class="overflow-x-scroll font-mono"><span></span><span class="kn">import</span> <span class="nn">deepchem</span> <span class="k">as</span> <span class="nn">dc</span>\n<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>\n\n\n<span class="k">class</span> <span class="nc">PongEnv</span><span class="p">(</span><span class="n">dc</span><span class="o">.</span><span class="n">rl</span><span class="o">.</span><span class="n">GymEnvironment</span><span class="p">):</span>\n  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>\n    <span class="nb">super</span><span class="p">(</span><span class="n">PongEnv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s1">\'Pong-v4\'</span><span class="p">)</span>\n    <span class="bp">self</span><span class="o">.</span><span class="n">_state_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">80</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>\n\n  <span class="nd">@property</span>\n  <span class="k">def</span> <span class="nf">state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>\n    <span class="c1"># Crop everything outside the play area, reduce the image size,</span>\n    <span class="c1"># and convert it to black and white.</span>\n    <span class="n">state_array</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span>\n    <span class="n">cropped</span> <span class="o">=</span> <span class="n">state_array</span><span class="p">[</span><span class="mi">34</span><span class="p">:</span><span class="mi">194</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>\n    <span class="n">reduced</span> <span class="o">=</span> <span class="n">cropped</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>\n    <span class="n">grayscale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">reduced</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>\n    <span class="n">bw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">grayscale</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>\n    <span class="n">bw</span><span class="p">[</span><span class="n">grayscale</span> <span class="o">!=</span> <span class="mi">233</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>\n    <span class="k">return</span> <span class="n">bw</span>\n\n  <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">):</span>\n    <span class="k">return</span> <span class="n">PongEnv</span><span class="p">()</span>\n\n<span class="n">env</span> <span class="o">=</span> <span class="n">PongEnv</span><span class="p">()</span>\n</pre>\n      </div>\n     </div>\n    </div>\n   </div>\n  </div>\n </div>\n <div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">\n  <div class="jp-Cell-inputWrapper" tabindex="0">\n   <div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">\n   </div>\n   <div class="jp-InputArea jp-Cell-inputArea">\n    <div class="jp-InputPrompt jp-InputArea-prompt">\n    </div>\n    <div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">\n     <p>\n      Next we create a model to implement our policy.  This model receives the current state of the environment (the pixels being displayed on the screen at this moment) as its input.  Given that input, it decides what action to perform.  In Pong there are three possible actions at any moment: move the paddle up, move it down, or leave it where it is.  The policy model produces a probability distribution over these actions.  It also produces a\n      <em>\n       value\n      </em>\n      output, which is interpreted as an estimate of how good the current state is.  This turns out to be important for efficient learning.\n     </p>\n     <p>\n      The model begins with two convolutional layers to process the image.  That is followed by a dense (fully connected) layer to provide plenty of capacity for game logic.  We also add a small Gated Recurrent Unit (GRU).  That gives the network a little bit of memory, so it can keep track of which way the ball is moving.  Just from the screen image, you cannot tell whether the ball is moving to the left or to the right, so having memory is important.\n     </p>\n     <p>\n      We concatenate the dense and GRU outputs together, and use them as inputs to two final layers that serve as the\nnetwork\'s outputs.  One computes the action probabilities, and the other computes an estimate of the\nstate value function.\n     </p>\n     <p>\n      We also provide an input for the initial state of the GRU, and return its final state at the end.  This is required by the learning algorithm.\n     </p>\n    </div>\n   </div>\n  </div>\n </div>\n <div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">\n  <div class="jp-Cell-inputWrapper" tabindex="0">\n   <div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">\n   </div>\n   <div class="jp-InputArea jp-Cell-inputArea">\n    <div class="jp-InputPrompt jp-InputArea-prompt">\n     In\xa0[4]:\n    </div>\n    <div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">\n     <div class="cm-editor cm-s-jupyter">\n      <div class="highlight hl-ipython3">\n       <pre class="overflow-x-scroll font-mono"><span></span><span class="kn">import</span> <span class="nn">torch</span>\n<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>\n<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>\n\n<span class="k">class</span> <span class="nc">PongPolicy</span><span class="p">(</span><span class="n">dc</span><span class="o">.</span><span class="n">rl</span><span class="o">.</span><span class="n">Policy</span><span class="p">):</span>\n    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>\n        <span class="nb">super</span><span class="p">(</span><span class="n">PongPolicy</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">([</span><span class="s1">\'action_prob\'</span><span class="p">,</span> <span class="s1">\'value\'</span><span class="p">,</span> <span class="s1">\'rnn_state\'</span><span class="p">],</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)])</span>\n\n    <span class="k">def</span> <span class="nf">create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>\n        <span class="k">class</span> <span class="nc">TestModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>\n            <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>\n                <span class="nb">super</span><span class="p">(</span><span class="n">TestModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>\n                <span class="c1"># Convolutional layers</span>\n                <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>\n                <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>\n                <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>\n                <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">batch_first</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>\n                <span class="bp">self</span><span class="o">.</span><span class="n">action_prob</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">272</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span>\n                <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">272</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>\n            <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>\n                <span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">((</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>\n                <span class="n">rnn_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>\n                <span class="n">reshaped</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>\n                <span class="n">conv1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">reshaped</span><span class="p">))</span>\n                <span class="n">conv2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">conv1</span><span class="p">))</span>\n                <span class="n">conv2</span> <span class="o">=</span> <span class="n">conv2</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">conv2</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>\n                <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">conv2</span><span class="p">))</span>\n                <span class="n">reshaped_x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>\n                <span class="c1">#x = torch.flatten(x, 1)</span>\n                <span class="n">gru_out</span><span class="p">,</span> <span class="n">rnn_final_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">reshaped_x</span><span class="p">,</span> <span class="n">rnn_state</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>\n                <span class="n">rnn_final_state</span> <span class="o">=</span> <span class="n">rnn_final_state</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)</span>\n                <span class="n">gru_out</span> <span class="o">=</span> <span class="n">gru_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>\n                <span class="n">concat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">gru_out</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>\n                <span class="c1">#concat = concat.view(-1, 272)</span>\n                <span class="n">action_prob</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_prob</span><span class="p">(</span><span class="n">concat</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>\n                <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">concat</span><span class="p">)</span>\n                <span class="k">return</span> <span class="n">action_prob</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">rnn_final_state</span>\n        <span class="k">return</span> <span class="n">TestModel</span><span class="p">()</span>\n<span class="n">policy</span> <span class="o">=</span> <span class="n">PongPolicy</span><span class="p">()</span>\n</pre>\n      </div>\n     </div>\n    </div>\n   </div>\n  </div>\n </div>\n <div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">\n  <div class="jp-Cell-inputWrapper" tabindex="0">\n   <div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">\n   </div>\n   <div class="jp-InputArea jp-Cell-inputArea">\n    <div class="jp-InputPrompt jp-InputArea-prompt">\n    </div>\n    <div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">\n     <p>\n      We will optimize the policy using the Advantage Actor Critic (A2C) algorithm.  There are lots of hyperparameters we could specify at this point, but the default values for most of them work well on this problem.  The only one we need to customize is the learning rate.\n     </p>\n    </div>\n   </div>\n  </div>\n </div>\n <div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">\n  <div class="jp-Cell-inputWrapper" tabindex="0">\n   <div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">\n   </div>\n   <div class="jp-InputArea jp-Cell-inputArea">\n    <div class="jp-InputPrompt jp-InputArea-prompt">\n     In\xa0[5]:\n    </div>\n    <div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">\n     <div class="cm-editor cm-s-jupyter">\n      <div class="highlight hl-ipython3">\n       <pre class="overflow-x-scroll font-mono"><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>\n<span class="kn">from</span> <span class="nn">deepchem.rl.torch_rl.torch_a2c</span> <span class="kn">import</span> <span class="n">A2C</span>\n\n<span class="kn">from</span> <span class="nn">deepchem.models.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>\n<span class="n">a2c</span> <span class="o">=</span> <span class="n">A2C</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">model_dir</span><span class="o">=</span><span class="s1">\'model\'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">))</span>\n</pre>\n      </div>\n     </div>\n    </div>\n   </div>\n  </div>\n </div>\n <div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">\n  <div class="jp-Cell-inputWrapper" tabindex="0">\n   <div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">\n   </div>\n   <div class="jp-InputArea jp-Cell-inputArea">\n    <div class="jp-InputPrompt jp-InputArea-prompt">\n    </div>\n    <div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">\n     <p>\n      Optimize for as long as you have patience to.  By 1 million steps you should see clear signs of learning.  Around 3 million steps it should start to occasionally beat the game\'s built in AI.  By 7 million steps it should be winning almost every time.  Running on my laptop, training takes about 20 minutes for every million steps.\n     </p>\n    </div>\n   </div>\n  </div>\n </div>\n <div class="jp-Cell jp-CodeCell jp-Notebook-cell">\n  <div class="jp-Cell-inputWrapper" tabindex="0">\n   <div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">\n   </div>\n   <div class="jp-InputArea jp-Cell-inputArea">\n    <div class="jp-InputPrompt jp-InputArea-prompt">\n     In\xa0[6]:\n    </div>\n    <div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">\n     <div class="cm-editor cm-s-jupyter">\n      <div class="highlight hl-ipython3">\n       <pre class="overflow-x-scroll font-mono"><span></span><span class="c1"># Change this to train as many steps as you have patience for.</span>\n<span class="n">a2c</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>\n</pre>\n      </div>\n     </div>\n    </div>\n   </div>\n  </div>\n  <div class="jp-Cell-outputWrapper">\n   <div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">\n   </div>\n   <div class="jp-OutputArea jp-Cell-outputArea">\n    <div class="jp-OutputArea-child">\n     <div class="jp-OutputPrompt jp-OutputArea-prompt">\n     </div>\n     <div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">\n      <pre class="overflow-x-scroll font-mono">c:\\Users\\HP\\anaconda3\\envs\\deep\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n  if not isinstance(terminated, (bool, np.bool8)):\n</pre>\n     </div>\n    </div>\n   </div>\n  </div>\n </div>\n <div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">\n  <div class="jp-Cell-inputWrapper" tabindex="0">\n   <div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">\n   </div>\n   <div class="jp-InputArea jp-Cell-inputArea">\n    <div class="jp-InputPrompt jp-InputArea-prompt">\n    </div>\n    <div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">\n     <p>\n      Let\'s watch it play and see how it does!\n     </p>\n    </div>\n   </div>\n  </div>\n </div>\n <div class="jp-Cell jp-CodeCell jp-Notebook-cell">\n  <div class="jp-Cell-inputWrapper" tabindex="0">\n   <div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">\n   </div>\n   <div class="jp-InputArea jp-Cell-inputArea">\n    <div class="jp-InputPrompt jp-InputArea-prompt">\n     In\xa0[7]:\n    </div>\n    <div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">\n     <div class="cm-editor cm-s-jupyter">\n      <div class="highlight hl-ipython3">\n       <pre class="overflow-x-scroll font-mono"><span></span><span class="c1"># This code doesn\'t work well on Colab</span>\n<span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>\n<span class="k">while</span> <span class="ow">not</span> <span class="n">env</span><span class="o">.</span><span class="n">terminated</span><span class="p">:</span>\n    <span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>\n    <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a2c</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">state</span><span class="p">))</span>\n</pre>\n      </div>\n     </div>\n    </div>\n   </div>\n  </div>\n  <div class="jp-Cell-outputWrapper">\n   <div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">\n   </div>\n   <div class="jp-OutputArea jp-Cell-outputArea">\n    <div class="jp-OutputArea-child">\n     <div class="jp-OutputPrompt jp-OutputArea-prompt">\n     </div>\n     <div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">\n      <pre class="overflow-x-scroll font-mono">c:\\Users\\HP\\anaconda3\\envs\\deep\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:289: UserWarning: <span class="ansi-yellow-fg">WARN: No render fps was declared in the environment (env.metadata[\'render_fps\'] is None or not defined), rendering may occur at inconsistent fps.</span>\n  logger.warn(\n</pre>\n     </div>\n    </div>\n   </div>\n  </div>\n </div>\n <div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">\n  <div class="jp-Cell-inputWrapper" tabindex="0">\n   <div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">\n   </div>\n   <div class="jp-InputArea jp-Cell-inputArea">\n    <div class="jp-InputPrompt jp-InputArea-prompt">\n    </div>\n    <div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">\n     <h1>\n      Congratulations! Time to join the Community!\n      <a class="anchor-link" href="#Congratulations!-Time-to-join-the-Community!">\n       \xb6\n      </a>\n     </h1>\n     <p>\n      Congratulations on completing this tutorial notebook! If you enjoyed working through the tutorial, and want to continue working with DeepChem, we encourage you to finish the rest of the tutorials in this series. You can also help the DeepChem community in the following ways:\n     </p>\n     <h2>\n      Star DeepChem on\n      <a href="https://github.com/deepchem/deepchem">\n       GitHub\n      </a>\n      <a class="anchor-link" href="#Star-DeepChem-on-GitHub">\n       \xb6\n      </a>\n     </h2>\n     <p>\n      This helps build awareness of the DeepChem project and the tools for open source drug discovery that we\'re trying to build.\n     </p>\n     <h2>\n      Join the DeepChem Gitter\n      <a class="anchor-link" href="#Join-the-DeepChem-Gitter">\n       \xb6\n      </a>\n     </h2>\n     <p>\n      The DeepChem\n      <a href="https://gitter.im/deepchem/Lobby">\n       Gitter\n      </a>\n      hosts a number of scientists, developers, and enthusiasts interested in deep learning for the life sciences. Join the conversation!\n     </p>\n    </div>\n   </div>\n  </div>\n </div>\n</main>\n'},c=n(7294),i=n(7466),o=n.n(i);let r=()=>((0,c.useEffect)(()=>{var s,a;null===(s=document.getElementsByClassName("scroll-nav")[0])||void 0===s||s.remove();let n=document.querySelector(".notebook"),p=document.querySelector(".notebook");p&&n&&o().init(n,{sections:"h1, h2",insertTarget:p,insertLocation:"after"}),null==MathJax||null===(a=MathJax.Hub)||void 0===a||a.Queue(["Typeset",MathJax.Hub])},[]),(0,p.jsx)("div",{className:"overflow-x-scroll",dangerouslySetInnerHTML:{__html:"".concat(t.html," ").concat(l.Z)}}));r.Layout=e.Z;var d=r}},function(s){s.O(0,[2443,9774,2888,179],function(){return s(s.s=3642)}),_N_E=s.O()}]);